{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZBADr90N+HDST1rLsHP3F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaystryukAY/SparkTask/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка, тест pySpark!\n"
      ],
      "metadata": {
        "id": "5WKuW4gokUdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIoj4dH5kUJ8",
        "outputId": "fee791e7-20f4-4e37-ba31-29c6b993522d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pip 24.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMuSrB75keN7",
        "outputId": "fca508d7-9b0d-450b-eb03-16853ddb9853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=673e012a01233f510aadcdd20b39d74b3624ed4aa70b43bae6cdf80843c11ae9\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "\n",
        "\n",
        "# Настройка Spark\n",
        "\n",
        "conf = SparkConf().setAppName(\"Simple RDD Example\").setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "\n",
        "\n",
        "# 1. Создание RDD из списка чисел\n",
        "\n",
        "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "rdd = sc.parallelize(numbers)\n",
        "\n",
        "\n",
        "\n",
        "# 2. Трансформации: Фильтрация чётных чисел\n",
        "\n",
        "even_numbers_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "\n",
        "\n",
        "# 3. Действие: Подсчёт суммы чётных чисел\n",
        "\n",
        "sum_even_numbers = even_numbers_rdd.sum()\n",
        "\n",
        "\n",
        "\n",
        "# Вывод результата\n",
        "\n",
        "print(\"Чётные числа:\", even_numbers_rdd.collect())\n",
        "\n",
        "print(\"Сумма чётных чисел:\", sum_even_numbers)\n",
        "\n",
        "\n",
        "\n",
        "a = 5 + 6\n",
        "\n",
        "print(a)\n",
        "\n",
        "# Остановка SparkContext\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr4Pur7Ykj34",
        "outputId": "1e4512d4-56ef-4f95-da97-9575e8f305d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Чётные числа: [2, 4, 6, 8, 10]\n",
            "Сумма чётных чисел: 30\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Сначала подключим SparkSession для DataFrame\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"FirstTicket\").getOrCreate()\n",
        "\n",
        "#Считаем данные из нашего файла\n",
        "df = spark.read.csv(\"/content/sample_data/weather_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "#Поле \"Дата\" не преобразую, тк оно и так имеет нужный формат.\n",
        "#df.printSchema()\n",
        "\n",
        "#Подключим SparkSQL\n",
        "df.createOrReplaceTempView(\"weather\")\n",
        "\n",
        "#Найдите топ-5 самых жарких дней за все время наблюдений. 3.1\n",
        "join_df = spark.sql(\"\"\"\n",
        "SELECT w.date, w.temperature\n",
        "FROM weather w\n",
        "ORDER BY w.temperature desc\n",
        "\"\"\")\n",
        "join_df.show(5)\n",
        "\n",
        "# Найдите метеостанцию с наибольшим количеством осадков за max год.\n",
        "join_df1 = spark.sql(\"\"\"\n",
        "SELECT station_id, SUM(precipitation)\n",
        "FROM weather\n",
        "WHERE year(date) == year(current_date())-1\n",
        "GROUP BY station_id\n",
        "ORDER BY SUM(precipitation) DESC\n",
        "LIMIT 1\n",
        "\"\"\")\n",
        "join_df1.show()\n",
        "\n",
        "# Подсчитайте среднюю температуру по месяцам за все время наблюдений\n",
        "join_df2 = spark.sql(\"\"\"\n",
        "SELECT month(date) as month, avg(temperature)\n",
        "FROM weather\n",
        "GROUP BY month(date)\n",
        "ORDER BY month(date)\n",
        "\"\"\")\n",
        "join_df2.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxg4yH6-W8X6",
        "outputId": "985d4801-7305-4624-a9fd-aebd132e4737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+\n",
            "|      date|       temperature|\n",
            "+----------+------------------+\n",
            "|2021-08-20|39.982828249354846|\n",
            "|2023-12-02| 39.96797489293784|\n",
            "|2022-03-28|  39.8246894248997|\n",
            "|2019-02-11| 39.76737697836647|\n",
            "|2020-06-10| 39.69147838355929|\n",
            "+----------+------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+----------+------------------+\n",
            "|station_id|sum(precipitation)|\n",
            "+----------+------------------+\n",
            "| station_5| 642.9302626767898|\n",
            "+----------+------------------+\n",
            "\n",
            "+-----+------------------+\n",
            "|month|  avg(temperature)|\n",
            "+-----+------------------+\n",
            "|    1|11.356518462550754|\n",
            "|    2| 9.067229891101926|\n",
            "|    3| 7.244080205633994|\n",
            "|    4|12.024529009744693|\n",
            "|    5| 9.902883346912718|\n",
            "|    6|13.421092297254138|\n",
            "|    7|6.1857183016954576|\n",
            "|    8|  10.9678002814186|\n",
            "|    9| 9.596744236573942|\n",
            "|   10|  9.09884344821895|\n",
            "|   11| 7.265889994697494|\n",
            "|   12|11.218592100674337|\n",
            "+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, mean, month, year, when\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SecondTicket\").getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "dfBook = spark.read.csv(\"/content/sample_data/books.csv\", header = True, inferSchema = True)\n",
        "dfAuth = spark.read.csv(\"/content/sample_data/authors.csv\", header = True, inferSchema = True)\n",
        "\n",
        "dfBook.createOrReplaceTempView(\"books\")\n",
        "dfAuth.createOrReplaceTempView(\"authors\")\n",
        "\n",
        "dfAuth = dfAuth.withColumn(\"birth_date\", to_date(col(\"birth_date\"), \"yyyy-MM-dd\"))\n",
        "dfBook = dfBook.withColumn(\"publish_date\", to_date(col(\"publish_date\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "join_df = spark.sql(\"\"\"\n",
        "SELECT *\n",
        "FROM books b\n",
        "JOIN authors a\n",
        "ON b.author_id = a.author_id\n",
        "\"\"\")\n",
        "\n",
        "#join_df.show()\n",
        "\n",
        "# Найдите топ-5 авторов, книги которых принесли наибольшую выручку.\n",
        "join_df1 = spark.sql(\"\"\"\n",
        "SELECT b.author_id, a.name, sum(b.price)\n",
        "FROM books b\n",
        "JOIN authors a\n",
        "ON b.author_id = a.author_id\n",
        "GROUP BY b.author_id, a.name\n",
        "ORDER BY sum(b.price) DESC\n",
        "LIMIT 5\n",
        "\"\"\")\n",
        "#join_df1.show()\n",
        "\n",
        "# Найдите количество книг в каждом жанре.\n",
        "join_df2 = spark.sql(\"\"\"\n",
        "SELECT b.genre, count(title) as count\n",
        "FROM books b\n",
        "JOIN authors a\n",
        "ON b.author_id = a.author_id\n",
        "GROUP BY b.genre\n",
        "ORDER BY count(title) desc\n",
        "\"\"\")\n",
        "#join_df2.show()\n",
        "\n",
        "# Подсчитайте среднюю цену книг по каждому автору.\n",
        "join_df3 = spark.sql(\"\"\"\n",
        "SELECT b.author_id, a.name, AVG(b.price) as average_price\n",
        "FROM books b\n",
        "JOIN authors a\n",
        "ON b.author_id = a.author_id\n",
        "GROUP BY b.author_id, a.name\n",
        "ORDER BY AVG(b.price) desc\n",
        "\"\"\")\n",
        "#join_df3.show()\n",
        "\n",
        "# Найдите книги, опубликованные после 2000 года, и отсортируйте их по цене.\n",
        "join_df4 = spark.sql(\"\"\"\n",
        "SELECT b.author_id, b.book_id, b.title, b.genre, b.price,\n",
        "       b.publish_date, a.name, a.birth_date, a.country\n",
        "FROM books b\n",
        "JOIN authors a\n",
        "ON b.author_id = a.author_id\n",
        "WHERE year(b.publish_date) >= \"2000\"\n",
        "GROUP BY b.author_id, b.book_id, b.title, b.genre, b.price,\n",
        "         b.publish_date, a.name, a.birth_date, a.country\n",
        "ORDER BY b.price desc\n",
        "\"\"\")\n",
        "#join_df4.show()"
      ],
      "metadata": {
        "id": "uYDRMw4UE9cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, mean, month, year, when\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ThreeTicket\").getOrCreate()\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "\n",
        "dfActor = spark.read.csv(\"/content/sample_data/actors.csv\", header = True, inferSchema = True)\n",
        "dfMovies = spark.read.csv(\"/content/sample_data/movies.csv\", header = True, inferSchema = True)\n",
        "dfMovieAct = spark.read.csv(\"/content/sample_data/movie_actors.csv\", header = True, inferSchema = True)\n",
        "\n",
        "dfActor.createOrReplaceTempView(\"actors\")\n",
        "dfMovies.createOrReplaceTempView(\"movies\")\n",
        "dfMovieAct.createOrReplaceTempView(\"movie_actors\")\n",
        "\n",
        "# join_df = spark.sql(\"\"\"\n",
        "# SELECT *\n",
        "# FROM movie_actors ma\n",
        "# JOIN actors a\n",
        "# ON  ma.actor_id = a.actor_id\n",
        "# JOIN movies m\n",
        "# ON ma.movie_id = m.movie_id\n",
        "# \"\"\")\n",
        "#join_df.show()\n",
        "\n",
        "# Найдите топ-5 жанров по количеству фильмов.\n",
        "join_df1 = spark.sql(\"\"\"\n",
        "SELECT m.genre, count(*) as num_movies\n",
        "FROM movies m\n",
        "GROUP BY m.genre\n",
        "ORDER BY num_movies DESC\n",
        "LIMIT 5\n",
        "\"\"\")\n",
        "#join_df1.show()\n",
        "\n",
        "# Найдите актера с наибольшим количеством фильмов.\n",
        "join_df2 = spark.sql(\"\"\"\n",
        "SELECT a.name, count(ma.movie_id) as num_movies\n",
        "FROM movie_actors ma\n",
        "JOIN actors a\n",
        "ON  ma.actor_id = a.actor_id\n",
        "GROUP BY a.name\n",
        "ORDER BY count(ma.movie_id) DESC\n",
        "LIMIT 1\n",
        "\"\"\")\n",
        "#join_df2.show()\n",
        "\n",
        "# Подсчитайте средний бюджет фильмов по жанрам.\n",
        "join_df3 = spark.sql(\"\"\"\n",
        "SELECT m.genre, avg(budget) as avg_budget\n",
        "FROM movies m\n",
        "GROUP BY m.genre\n",
        "\"\"\")\n",
        "#join_df3.show()\n",
        "\n",
        "# Найдите фильмы, в которых снялось более одного актера из одной страны.\n",
        "join_df4 = spark.sql(\"\"\"\n",
        "SELECT m.title, a.country, count(a.actor_id) as num_actors\n",
        "FROM movie_actors ma\n",
        "JOIN actors a\n",
        "ON  ma.actor_id = a.actor_id\n",
        "JOIN movies m\n",
        "ON ma.movie_id = m.movie_id\n",
        "GROUP BY m.title, a.country\n",
        "HAVING count(a.actor_id) > 1\n",
        "\"\"\").show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wps4J2bBQpYA",
        "outputId": "3d7d97bd-5eb8-44a2-bd8f-0e6865dbe4a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+----------+\n",
            "|   title|  country|num_actors|\n",
            "+--------+---------+----------+\n",
            "| Movie_7|    India|         2|\n",
            "| Movie_3|      USA|         2|\n",
            "|Movie_10|       UK|         2|\n",
            "|Movie_15|    India|         2|\n",
            "|Movie_18|Australia|         2|\n",
            "| Movie_1|    India|         3|\n",
            "| Movie_2|      USA|         2|\n",
            "| Movie_7|      USA|         2|\n",
            "|Movie_10|      USA|         2|\n",
            "+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Итоговое задание  №4\n",
        "!pip install faker\n",
        "\n",
        "import csv\n",
        "from faker import Faker\n",
        "import random\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "num_records = 100000\n",
        "\n",
        "http_methods = ['GET', 'POST', 'PUT', 'DELETE']\n",
        "response_codes = [200, 301, 404, 500]\n",
        "\n",
        "file_path = \"web_server_logs.csv\"\n",
        "\n",
        "with open(file_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['ip', 'timestamp', 'method', 'url', 'response_code', 'response_size'])\n",
        "\n",
        "    for _ in range(num_records):\n",
        "        ip = fake.ipv4()\n",
        "        timestamp = fake.date_time_this_year().isoformat()\n",
        "        method = random.choice(http_methods)\n",
        "        url = fake.uri_path()\n",
        "        response_code = random.choice(response_codes)\n",
        "        response_size = random.randint(100, 10000)\n",
        "\n",
        "        writer.writerow([ip, timestamp, method, url, response_code, response_size])\n",
        "\n",
        "print(f\"Сгенерировано {num_records} записей и сохранено в {file_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGz6WeIkZL1n",
        "outputId": "f4b95be1-36d9-41ed-b200-92b3d69b9933"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (28.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Сгенерировано 100000 записей и сохранено в web_server_logs.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_date, mean, month, year, when\n",
        "\n",
        "spark = SparkSession.builder.appName('read').getOrCreate()\n",
        "\n",
        "df_logs = spark.read.csv('web_server_logs.csv', header=True, inferSchema=True)\n",
        "\n",
        "\"\"\"\n",
        "Создал отдельный dataframe и таблицу для удобства, чтобы попробовать решить задачи разными подходами,\n",
        "например 1 и 2 задачу так скажем в \"sql стиле\", а 3 и 4 в \"пайтон стиле\" :)\n",
        "Самому гораздо удобнее использовать \"sql стиль\", но 3 задача очень лаконично выглядит в \"пайтон стиле\"\n",
        " \"\"\"\n",
        "\n",
        "df = spark.read.csv('web_server_logs.csv', header=True, inferSchema=True)\n",
        "df.createOrReplaceTempView(\"logs\")\n",
        "\n",
        "# 1 Сгруппируйте данные по IP и посчитайте количество запросов для каждого IP, выводим 10 самых активных IP.\n",
        "df = spark.sql(\"\"\"\n",
        "SELECT l.ip, count(*) as request_count\n",
        "FROM logs l\n",
        "GROUP BY l.ip\n",
        "ORDER BY count(l.ip) desc\n",
        "LIMIT 10\n",
        "\"\"\").show()\n",
        "\n",
        "# 2 Сгруппируйте данные по HTTP-методу и посчитайте количество запросов для каждого метода.\n",
        "df = spark.sql(\"\"\"\n",
        "SELECT l.method, count(*) as method_count\n",
        "FROM logs l\n",
        "GROUP BY l.method\n",
        "ORDER BY count(*) desc --хоть и не было условия на сортировку, но так красивее :)\n",
        "\"\"\").show()\n",
        "\n",
        "# 3 Профильтруйте и посчитайте количество запросов с кодом ответа 404.\n",
        "count_errors = (\n",
        "    df_logs\n",
        "        .filter(col('response_code') == 404)\n",
        "        .count()\n",
        ")\n",
        "print(f'Number of 404 response codes: {count_errors}')\n",
        "\n",
        "# 4 Сгруппируйте данные по дате и просуммируйте размер ответов, сортируйте по дате.\n",
        "(\n",
        "    df_logs\n",
        "        .groupBy(to_date(col('timestamp')).alias('date'))\n",
        "        .agg({'response_size': 'sum'})\n",
        "        .withColumnRenamed('sum(response_size)', 'total_response_size')\n",
        "        .orderBy(col('date'))\n",
        ").show()\n",
        "\n",
        "spark.stop()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fKmNvGWq28l",
        "outputId": "47ea823d-6269-412e-ce1d-eb8005143d1f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "+--------------+-------------+\n",
            "|            ip|request_count|\n",
            "+--------------+-------------+\n",
            "| 50.213.38.181|            2|\n",
            "|  155.72.18.30|            2|\n",
            "|   33.162.59.4|            1|\n",
            "|   57.0.39.236|            1|\n",
            "|169.91.193.185|            1|\n",
            "|214.214.72.250|            1|\n",
            "|219.180.47.177|            1|\n",
            "| 48.162.111.29|            1|\n",
            "|  56.53.119.46|            1|\n",
            "|20.169.132.224|            1|\n",
            "+--------------+-------------+\n",
            "\n",
            "+------+------------+\n",
            "|method|method_count|\n",
            "+------+------------+\n",
            "|  POST|       25172|\n",
            "|   PUT|       24994|\n",
            "|   GET|       24989|\n",
            "|DELETE|       24845|\n",
            "+------+------------+\n",
            "\n",
            "Number of 404 response codes: 25018\n",
            "+----------+-------------------+\n",
            "|      date|total_response_size|\n",
            "+----------+-------------------+\n",
            "|2024-01-01|            1990489|\n",
            "|2024-01-02|            1797495|\n",
            "|2024-01-03|            2019891|\n",
            "|2024-01-04|            1967152|\n",
            "|2024-01-05|            2228000|\n",
            "|2024-01-06|            1861014|\n",
            "|2024-01-07|            1971706|\n",
            "|2024-01-08|            1999713|\n",
            "|2024-01-09|            1878646|\n",
            "|2024-01-10|            1870385|\n",
            "|2024-01-11|            1816006|\n",
            "|2024-01-12|            1961107|\n",
            "|2024-01-13|            2053117|\n",
            "|2024-01-14|            2077495|\n",
            "|2024-01-15|            1930719|\n",
            "|2024-01-16|            1847616|\n",
            "|2024-01-17|            1833542|\n",
            "|2024-01-18|            1815231|\n",
            "|2024-01-19|            2054088|\n",
            "|2024-01-20|            1814203|\n",
            "+----------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}